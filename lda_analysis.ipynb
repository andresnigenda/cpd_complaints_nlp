{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lda_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhR4n5unMwnQhYPrlILiSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andresnigenda/cpd_complaints_nlp/blob/andres/lda_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG7Wsr5JR_ZW",
        "colab_type": "text"
      },
      "source": [
        "# **LDA of complaints against the CPD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZKVIPm4RgZq",
        "colab_type": "text"
      },
      "source": [
        "### *Importing Packages*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdsgV4neHweG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "95e3f8c6-3a0d-441d-e33d-9b8908ad83bd"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction import text \n",
        "from spacy.tokenizer import Tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from spacy.lang.en import English\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "from nltk import word_tokenize\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffM_7r46SvlL",
        "colab_type": "text"
      },
      "source": [
        "### *Pre-processing*\n",
        "0. Set up allegations data from csv\n",
        "1. Convert data to lowercase\n",
        "2. Remove special characters (punctuation and numbers)\n",
        "3. Tokenize into terms\n",
        "4. Remove stop words (generic + allegation specific)\n",
        "5. Stemming\n",
        "6. Term document matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbUzq1roSvBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "6983c043-02d8-4ad9-d953-616a15940e32"
      },
      "source": [
        "# 0. Set up allegations data from csv #\n",
        "#\n",
        "# read in csv\n",
        "narratives_csv_url = \"https://raw.githubusercontent.com/andresnigenda/cpd_complaints_nlp/andres/narratives.csv\"\n",
        "df = pd.read_csv(narratives_csv_url)\n",
        "# filter to relevant section\n",
        "df = df[df.column_name == \"Initial / Intake Allegation\"]\n",
        "# filter to relevant columns\n",
        "df = df[['cr_id', 'text']]\n",
        "print(\"There are {} complaints\".format(df.shape[0]))\n",
        "# drop allegations with same id + text\n",
        "df = df.drop_duplicates(['cr_id', 'text'])\n",
        "print(\"There are {} unique complaints\".format(df.shape[0]))\n",
        "#allegations_lst = df['text'].to_list()\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 19966 complaints\n",
            "There are 17001 unique complaints\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cr_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1048960</td>\n",
              "      <td>The reporting party alleges that the\\naccused ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1048962</td>\n",
              "      <td>The victim alleges that an unknown male\\nblack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1048964</td>\n",
              "      <td>The reporting party alleges that he was a\\nvi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1048965</td>\n",
              "      <td>The reporting party alleges that while\\nwaitin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1048965</td>\n",
              "      <td>The reporting party alleges that while\\nwaitin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cr_id                                               text\n",
              "0   1048960  The reporting party alleges that the\\naccused ...\n",
              "4   1048962  The victim alleges that an unknown male\\nblack...\n",
              "9   1048964   The reporting party alleges that he was a\\nvi...\n",
              "12  1048965  The reporting party alleges that while\\nwaitin...\n",
              "14  1048965  The reporting party alleges that while\\nwaitin..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQVPsaTEkxsF",
        "colab_type": "text"
      },
      "source": [
        "We want to look at categories like: nudity_penetration,sexual_relations_with_a_minor_,sexual_harassment_sexual_remarks,domestic_violence_police_committing_,sexual_humiliation_sexual_extortion_prostitution_sex_work,tasers_baton_aggressive_physical_touch_gun,trespass_robbery,biometric_surveillance_fitting_a_description_gang_related_,racial_slurs_xenophobic_remarks_,undocumented_status_asking_for_someone_s_status_calling_ice_,planting_drug_guns,neglect_of_duty_failure_to_serve,refusing_to_provide_medical_assistance,workplace_harassment,_irrational_aggressive_unstable_,suicide_in_jail_improper_care_,dcfs_threats,pregnant_women,school,searching_patting_down_arresting_minors\n",
        "\n",
        "So we will eliminate words like accused, reporting, party, alleges, officer, alleged, complainant, officers, victim, vehicle, failed, police, justification, stated, report, states, called, did, unknown, told, provide, incident, regarding, issued."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pXQhb9fFxp2x",
        "colab": {}
      },
      "source": [
        "class PreProcess():\n",
        "  '''\n",
        "  Class for pre-processing a csv of text documents into a  sparse matrix of\n",
        "  counts following scikit-learn's CountVectorizer\n",
        "\n",
        "  Source:\n",
        "  https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\n",
        "  '''\n",
        "\n",
        "  def __init__(self, raw_data, additional_stopwords, stem=True):\n",
        "      self.raw_data = raw_data\n",
        "      self.stemmer = PorterStemmer()\n",
        "      self.stop_words = set(nltk.corpus.stopwords.words('english')).union(additional_stopwords)\n",
        "      self.stem = stem\n",
        "      self.vectorizer = None\n",
        "      self.doc_term_matrix = None\n",
        "\n",
        "\n",
        "  def _tokenize_text(self, text):\n",
        "      '''\n",
        "      Strips punctuation, and everything that isn't alphabetic, tokenizes.\n",
        "      Stems by default.\n",
        "      '''\n",
        "      tokenized_text = []\n",
        "      \n",
        "      # drop whatever isn't a word with letters or an apostrophe\n",
        "      for token in word_tokenize(text):\n",
        "        # to lowercase\n",
        "        token = token.lower()\n",
        "        # substitute whatever is not alphabetic\n",
        "        token = re.sub('[^a-z]', '', token)\n",
        "        if token:\n",
        "          if token not in self.stop_words:\n",
        "            if self.stem:\n",
        "              tokenized_text.append(self.stemmer.stem(token))\n",
        "            else:\n",
        "              tokenized_text.append(token)\n",
        "      \n",
        "      return tokenized_text\n",
        "  \n",
        "  def _vectorize(self):\n",
        "      '''\n",
        "      Launch a vectorizer with CountVectorizer\n",
        "      '''\n",
        "      # instantiate vectorizer w/ our custom analyzer\n",
        "      # by default we drop words that appear in more than 80% of documents and\n",
        "      # that don't appear in more than one document\n",
        "      # we override the analyzer with our tokenizer method\n",
        "      self.vectorizer = CountVectorizer(max_df=0.8, \n",
        "                                        min_df=2, \n",
        "                                        analyzer=self._tokenize_text)\n",
        "\n",
        "  def _fit_vectorizer(self):\n",
        "      '''\n",
        "      Fit vectorizer and create a doc_term_matrix\n",
        "      '''\n",
        "      # launch the CountVectorizer object\n",
        "      self._vectorize()\n",
        "      # fit it\n",
        "      self.doc_term_matrix = self.vectorizer.fit_transform(self.raw_data.text.values.astype('U'))\n",
        "\n",
        "  def plot_word_distributions(self, N):\n",
        "      '''\n",
        "      Plots frequencies for top N words\n",
        "\n",
        "      Source:\n",
        "        - https://altair-viz.github.io/gallery/percentage_of_total.html\n",
        "      '''\n",
        "      # get word list\n",
        "      if not self.doc_term_matrix:\n",
        "        self._fit_vectorizer()\n",
        "\n",
        "      word_lst = self.vectorizer.get_feature_names()\n",
        "      counts_lst = np.asarray(self.doc_term_matrix.sum(axis=0)).tolist()[0]\n",
        "\n",
        "      source = pd.DataFrame({'Word': word_lst, 'Count': counts_lst}).sort_values(by=['Count'], ascending=False)[:N]\n",
        "      \n",
        "      alt.data_transformers.disable_max_rows()\n",
        "\n",
        "      plot = alt.Chart(source).transform_joinaggregate(\n",
        "          TotalCount='sum(Count)',\n",
        "      ).transform_calculate(\n",
        "          PercentOfTotal=\"datum.Count / datum.TotalCount\"\n",
        "      ).mark_bar().encode(\n",
        "          alt.X('PercentOfTotal:Q', axis=alt.Axis(format='.0%')),\n",
        "          alt.Y('Word:N', sort='-x')\n",
        "      )\n",
        "      return plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaVwXBAtDpae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "additional_stopwords = set([\"accused\", \"reporting\", \"party\", \"alleges\", \"officer\", \n",
        "                        \"alleged\", \"alleges\", \"complainant\", \"officers\", \"victim\", \n",
        "                        \"police\", \"stated\", \"report\", \"states\", \"called\", \n",
        "                        \"did\", \"told\", \"provide\", \"incident\", \"regarding\", \"issued\",\n",
        "                        \"reported\", \"vehicle\", \"car\", \"justification\",\n",
        "                        \"district\", \"uniformed\", \"threatened\", \"witness\", \"th\",\n",
        "                        \"number\", \"scene\"]).union(text.ENGLISH_STOP_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BMJkKYOHgfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "bce32361-1c0a-417c-b477-215301b2d755"
      },
      "source": [
        "# with stemming\n",
        "stemmed_data = PreProcess(df, additional_stopwords, True)\n",
        "stemmed_data.plot_word_distributions(20)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-106f098aa8734ab289059328cf3516dd\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-106f098aa8734ab289059328cf3516dd\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-106f098aa8734ab289059328cf3516dd\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-155c57d7bead33861f4d9a6d46334d0f\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-155c57d7bead33861f4d9a6d46334d0f\": [{\"Word\": \"fail\", \"Count\": 4810}, {\"Word\": \"arrest\", \"Count\": 4501}, {\"Word\": \"search\", \"Count\": 2930}, {\"Word\": \"stop\", \"Count\": 2869}, {\"Word\": \"refus\", \"Count\": 2682}, {\"Word\": \"male\", \"Count\": 2580}, {\"Word\": \"citat\", \"Count\": 2547}, {\"Word\": \"unknown\", \"Count\": 2145}, {\"Word\": \"fals\", \"Count\": 1974}, {\"Word\": \"rude\", \"Count\": 1940}, {\"Word\": \"time\", \"Count\": 1929}, {\"Word\": \"inform\", \"Count\": 1751}, {\"Word\": \"resid\", \"Count\": 1745}, {\"Word\": \"respond\", \"Count\": 1723}, {\"Word\": \"traffic\", \"Count\": 1716}, {\"Word\": \"white\", \"Count\": 1505}, {\"Word\": \"unprofession\", \"Count\": 1467}, {\"Word\": \"fuck\", \"Count\": 1437}, {\"Word\": \"return\", \"Count\": 1308}, {\"Word\": \"harass\", \"Count\": 1284}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GResRMhPvQwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a0fee14-1806-4167-c917-6c0b20face64"
      },
      "source": [
        "# without stemming\n",
        "non_stemmed_data = PreProcess(df, additional_stopwords, False)\n",
        "non_stemmed_data.plot_word_distributions(50)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-59976e9e86794350bb0c2676d61eb1f0\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-59976e9e86794350bb0c2676d61eb1f0\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-59976e9e86794350bb0c2676d61eb1f0\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-707d58b41817de738d9c33417e4815be\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-707d58b41817de738d9c33417e4815be\": [{\"Word\": \"failed\", \"Count\": 4751}, {\"Word\": \"refused\", \"Count\": 2600}, {\"Word\": \"male\", \"Count\": 2502}, {\"Word\": \"arrest\", \"Count\": 2305}, {\"Word\": \"unknown\", \"Count\": 2145}, {\"Word\": \"searched\", \"Count\": 2053}, {\"Word\": \"arrested\", \"Count\": 2029}, {\"Word\": \"rude\", \"Count\": 1898}, {\"Word\": \"citation\", \"Count\": 1791}, {\"Word\": \"stopped\", \"Count\": 1755}, {\"Word\": \"traffic\", \"Count\": 1716}, {\"Word\": \"residence\", \"Count\": 1538}, {\"Word\": \"white\", \"Count\": 1505}, {\"Word\": \"unprofessional\", \"Count\": 1451}, {\"Word\": \"time\", \"Count\": 1292}, {\"Word\": \"station\", \"Count\": 1274}, {\"Word\": \"black\", \"Count\": 1130}, {\"Word\": \"female\", \"Count\": 1114}, {\"Word\": \"case\", \"Count\": 1091}, {\"Word\": \"responded\", \"Count\": 1058}, {\"Word\": \"falsely\", \"Count\": 1019}, {\"Word\": \"property\", \"Count\": 1011}, {\"Word\": \"stop\", \"Count\": 998}, {\"Word\": \"information\", \"Count\": 998}, {\"Word\": \"return\", \"Count\": 991}, {\"Word\": \"reason\", \"Count\": 979}, {\"Word\": \"phone\", \"Count\": 967}, {\"Word\": \"warrant\", \"Count\": 962}, {\"Word\": \"false\", \"Count\": 955}, {\"Word\": \"son\", \"Count\": 930}, {\"Word\": \"license\", \"Count\": 913}, {\"Word\": \"fuck\", \"Count\": 879}, {\"Word\": \"asked\", \"Count\": 879}, {\"Word\": \"nt\", \"Count\": 879}, {\"Word\": \"offender\", \"Count\": 859}, {\"Word\": \"door\", \"Count\": 853}, {\"Word\": \"entered\", \"Count\": 841}, {\"Word\": \"home\", \"Count\": 840}, {\"Word\": \"inventory\", \"Count\": 839}, {\"Word\": \"going\", \"Count\": 812}, {\"Word\": \"accident\", \"Count\": 790}, {\"Word\": \"file\", \"Count\": 764}, {\"Word\": \"citations\", \"Count\": 756}, {\"Word\": \"search\", \"Count\": 750}, {\"Word\": \"sergeant\", \"Count\": 739}, {\"Word\": \"went\", \"Count\": 721}, {\"Word\": \"supervisor\", \"Count\": 714}, {\"Word\": \"driver\", \"Count\": 713}, {\"Word\": \"partyvictim\", \"Count\": 706}, {\"Word\": \"subject\", \"Count\": 702}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62IVHXpbC2n3",
        "colab_type": "text"
      },
      "source": [
        "### *Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZz_UFGC6OV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "74881311-57dd-4534-d062-6ae32799da03"
      },
      "source": [
        "LDA = LatentDirichletAllocation(n_components=30, random_state=8)\n",
        "LDA.fit(stemmed_data.doc_term_matrix)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=30, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=8, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TdqpEXBEQJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6771b214-e54f-494a-d8bf-12ed790b489f"
      },
      "source": [
        "for i, topic in enumerate(LDA.components_):\n",
        "    print(f'Top 10 words for topic {i + 1}:')\n",
        "    print([stemmed_data.vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "    print('\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words for topic 1:\n",
            "['plaintiff', 'charg', 'impound', 'probabl', 'plant', 'possess', 'caus', 'drug', 'fals', 'arrest']\n",
            "\n",
            "\n",
            "Top 10 words for topic 2:\n",
            "['handcuf', 'stop', 'femal', 'aggress', 'white', 'possibl', 'manner', 'male', 'appar', 'reason']\n",
            "\n",
            "\n",
            "Top 10 words for topic 3:\n",
            "['refus', 'resid', 'assault', 'secur', 'assist', 'servic', 'action', 'time', 'fail', 'respond']\n",
            "\n",
            "\n",
            "Top 10 words for topic 4:\n",
            "['radio', 'miss', 'indebted', 'properti', 'unknown', 'inform', 'citi', 'member', 'chicago', 'depart']\n",
            "\n",
            "\n",
            "Top 10 words for topic 5:\n",
            "['black', 'court', 'order', 'number', 'male', 'possibl', 'name', 'refus', 'request', 'star']\n",
            "\n",
            "\n",
            "Top 10 words for topic 6:\n",
            "['listen', 'spoke', 'ask', 'nt', 'refus', 'disrespect', 'traffic', 'yell', 'unprofession', 'rude']\n",
            "\n",
            "\n",
            "Top 10 words for topic 7:\n",
            "['rd', 'inform', 'quot', 'assign', 'thorough', 'conduct', 'case', 'detect', 'fail', 'investig']\n",
            "\n",
            "\n",
            "Top 10 words for topic 8:\n",
            "['state', 'physic', 'nigger', 'alterc', 'bitch', 'fuck', 'ass', 'refer', 'abus', 'verbal']\n",
            "\n",
            "\n",
            "Top 10 words for topic 9:\n",
            "['time', 'father', 'refus', 'batter', 'domest', 'offend', 'arrest', 'batteri', 'fail', 'son']\n",
            "\n",
            "\n",
            "Top 10 words for topic 10:\n",
            "['plaincloth', 'supervisor', 'fail', 'hispan', 'femal', 'black', 'unknown', 'subject', 'white', 'male']\n",
            "\n",
            "\n",
            "Top 10 words for topic 11:\n",
            "['profil', 'dealer', 'cross', 'detain', 'unknown', 'racial', 'citi', 'drug', 'sticker', 'follow']\n",
            "\n",
            "\n",
            "Top 10 words for topic 12:\n",
            "['famili', 'son', 'live', 'apart', 'look', 'come', 'enter', 'hous', 'resid', 'home']\n",
            "\n",
            "\n",
            "Top 10 words for topic 13:\n",
            "['drove', 'red', 'truck', 'emerg', 'stop', 'drive', 'activ', 'tow', 'sign', 'light']\n",
            "\n",
            "\n",
            "Top 10 words for topic 14:\n",
            "['area', 'go', 'continu', 'past', 'relat', 'arrest', 'time', 'stop', 'partyvictim', 'harass']\n",
            "\n",
            "\n",
            "Top 10 words for topic 15:\n",
            "['arrest', 'file', 'complaint', 'sister', 'text', 'refus', 'messag', 'cta', 'bu', 'mother']\n",
            "\n",
            "\n",
            "Top 10 words for topic 16:\n",
            "['miss', 'custodi', 'gun', 'releas', 'fail', 'retriev', 'bag', 'medic', 'person', 'properti']\n",
            "\n",
            "\n",
            "Top 10 words for topic 17:\n",
            "['wallet', 'took', 'money', 'key', 'remov', 'card', 'usc', 'fail', 'inventori', 'return']\n",
            "\n",
            "\n",
            "Top 10 words for topic 18:\n",
            "['refus', 'insur', 'plate', 'issu', 'stop', 'traffic', 'fals', 'driver', 'licens', 'citat']\n",
            "\n",
            "\n",
            "Top 10 words for topic 19:\n",
            "['gun', 'direct', 'said', 'profan', 'ask', 'nt', 'point', 'shut', 'weapon', 'fuck']\n",
            "\n",
            "\n",
            "Top 10 words for topic 20:\n",
            "['complaint', 'file', 'involv', 'assist', 'traffic', 'went', 'inform', 'refus', 'accid', 'station']\n",
            "\n",
            "\n",
            "Top 10 words for topic 21:\n",
            "['resid', 'brother', 'allow', 'hour', 'respond', 'locat', 'unknown', 'refus', 'duti', 'sergeant']\n",
            "\n",
            "\n",
            "Top 10 words for topic 22:\n",
            "['arrest', 'partner', 'person', 'unabl', 'husband', 'provid', 'fals', 'protect', 'inform', 'order']\n",
            "\n",
            "\n",
            "Top 10 words for topic 23:\n",
            "['away', 'arm', 'ground', 'threw', 'handcuf', 'struck', 'push', 'squad', 'drove', 'grab']\n",
            "\n",
            "\n",
            "Top 10 words for topic 24:\n",
            "['approxim', 'beat', 'arriv', 'suspect', 'telephon', 'identifi', 'unknown', 'arrest', 'hour', 'boyfriend']\n",
            "\n",
            "\n",
            "Top 10 words for topic 25:\n",
            "['drove', 'badg', 'exit', 'ask', 'turn', 'drive', 'friend', 'stop', 'pull', 'traffic']\n",
            "\n",
            "\n",
            "Top 10 words for topic 26:\n",
            "['write', 'street', 'vehicl', 'fals', 'cellular', 'cell', 'ticket', 'citat', 'phone', 'park']\n",
            "\n",
            "\n",
            "Top 10 words for topic 27:\n",
            "['execut', 'reason', 'handcuf', 'damag', 'permiss', 'enter', 'resid', 'warrant', 'stop', 'search']\n",
            "\n",
            "\n",
            "Top 10 words for topic 28:\n",
            "['falsifi', 'case', 'damag', 'malesubject', 'rd', 'arrest', 'open', 'taser', 'kick', 'door']\n",
            "\n",
            "\n",
            "Top 10 words for topic 29:\n",
            "['damag', 'neighbor', 'inform', 'refus', 'servic', 'file', 'case', 'arrest', 'offend', 'fail']\n",
            "\n",
            "\n",
            "Top 10 words for topic 30:\n",
            "['unknown', 'listen', 'time', 'girlfriend', 'refus', 'tint', 'right', 'window', 'dismiss', 'landlord']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}