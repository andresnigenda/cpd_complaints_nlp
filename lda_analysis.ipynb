{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lda_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+N6PlQ9hE4bPSKh8hq1Pk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andresnigenda/cpd_complaints_nlp/blob/andres/lda_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG7Wsr5JR_ZW",
        "colab_type": "text"
      },
      "source": [
        "# **LDA of complaints against the CPD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZKVIPm4RgZq",
        "colab_type": "text"
      },
      "source": [
        "### *Importing Packages*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdsgV4neHweG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction import text \n",
        "from spacy.tokenizer import Tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from spacy.lang.en import English\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "from nltk import word_tokenize\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffM_7r46SvlL",
        "colab_type": "text"
      },
      "source": [
        "### *Pre-processing*\n",
        "0. Set up allegations data from csv\n",
        "1. Convert data to lowercase\n",
        "2. Remove special characters (punctuation and numbers)\n",
        "3. Tokenize into terms\n",
        "4. Remove stop words (generic + allegation specific)\n",
        "5. Stemming\n",
        "6. Term document matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbUzq1roSvBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "47e1bd67-8575-4511-8414-1d8d7a84c8f6"
      },
      "source": [
        "# 0. Set up allegations data from csv #\n",
        "#\n",
        "# read in csv\n",
        "narratives_csv_url = \"https://raw.githubusercontent.com/andresnigenda/cpd_complaints_nlp/andres/narratives.csv\"\n",
        "df = pd.read_csv(narratives_csv_url)\n",
        "# filter to relevant section\n",
        "df = df[df.column_name == \"Initial / Intake Allegation\"]\n",
        "# filter to relevant columns\n",
        "df = df[['cr_id', 'text']]\n",
        "print(\"There are {} complaints\".format(df.shape[0]))\n",
        "# drop allegations with same id + text\n",
        "df = df.drop_duplicates(['cr_id', 'text'])\n",
        "print(\"There are {} unique complaints\".format(df.shape[0]))\n",
        "#allegations_lst = df['text'].to_list()\n",
        "df.head()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 19966 complaints\n",
            "There are 17001 unique complaints\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cr_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1048960</td>\n",
              "      <td>The reporting party alleges that the\\naccused ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1048962</td>\n",
              "      <td>The victim alleges that an unknown male\\nblack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1048964</td>\n",
              "      <td>The reporting party alleges that he was a\\nvi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1048965</td>\n",
              "      <td>The reporting party alleges that while\\nwaitin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1048965</td>\n",
              "      <td>The reporting party alleges that while\\nwaitin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cr_id                                               text\n",
              "0   1048960  The reporting party alleges that the\\naccused ...\n",
              "4   1048962  The victim alleges that an unknown male\\nblack...\n",
              "9   1048964   The reporting party alleges that he was a\\nvi...\n",
              "12  1048965  The reporting party alleges that while\\nwaitin...\n",
              "14  1048965  The reporting party alleges that while\\nwaitin..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0UnBQTJSsTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Convert to lowercase #\n",
        "#\n",
        "\n",
        "\n",
        "class PreProcess():\n",
        "  '''\n",
        "  Class for pre-processing a csv of text documents into a  sparse matrix of\n",
        "  counts following scikit-learn's CountVectorizer\n",
        "\n",
        "  Source:\n",
        "  https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\n",
        "  '''\n",
        "\n",
        "  def __init__(self, raw_data, stopwords=None):\n",
        "      self.raw_data = raw_data\n",
        "      self.stop_words = stopwords\n",
        "      self.vectorizer = None\n",
        "\n",
        "  def text_cleaner(self, text):\n",
        "      '''\n",
        "      Custom text preprocessor before tokenization. Converts data to lowercase and\n",
        "      removes special characters (punctuation and numbers)\n",
        "      '''\n",
        "      # conver to lowercase\n",
        "      text = text.lower()\n",
        "      # remove anything that is not a word\n",
        "      text = re.sub('[^a-z]', ' ', text)\n",
        "\n",
        "      return text\n",
        "\n",
        "  def _add_stopwords(self):\n",
        "      '''\n",
        "      Adds\n",
        "      '''\n",
        "      self.stop_words = text.ENGLISH_STOP_WORDS.union(self.stop_words)\n",
        "  \n",
        "  def _vectorize(self):\n",
        "      '''\n",
        "      Launch a vectorizer with CountVectorizer\n",
        "      '''\n",
        "      if self.stop_words:\n",
        "        self._add_stopwords()\n",
        "      self.vectorizer = CountVectorizer(max_df=0.8, \n",
        "                                        min_df=2, \n",
        "                                        preprocessor=self.text_cleaner,\n",
        "                                        stop_words=self.stop_words)\n",
        "\n",
        "  def _fit_vectorizer(self):\n",
        "      '''\n",
        "      Fit vectorizer\n",
        "      '''\n",
        "      self._vectorize()\n",
        "      X = self.vectorizer.fit_transform(self.raw_data.text.values.astype('U'))\n",
        "\n",
        "      return X\n",
        "\n",
        "  def plot_word_distributions(self, N):\n",
        "      '''\n",
        "      Plots frequencies for top N words\n",
        "\n",
        "      Source:\n",
        "        - https://altair-viz.github.io/gallery/percentage_of_total.html\n",
        "      '''\n",
        "      # get word list\n",
        "      X = self._fit_vectorizer()\n",
        "      word_lst = self.vectorizer.get_feature_names()\n",
        "      counts_lst = np.asarray(X.sum(axis=0)).tolist()[0]\n",
        "\n",
        "      source = pd.DataFrame({'Word': word_lst, 'Count': counts_lst}).sort_values(by=['Count'], ascending=False)[:N]\n",
        "      \n",
        "      alt.data_transformers.disable_max_rows()\n",
        "\n",
        "      plot = alt.Chart(source).transform_joinaggregate(\n",
        "          TotalCount='sum(Count)',\n",
        "      ).transform_calculate(\n",
        "          PercentOfTotal=\"datum.Count / datum.TotalCount\"\n",
        "      ).mark_bar().encode(\n",
        "          alt.X('PercentOfTotal:Q', axis=alt.Axis(format='.0%')),\n",
        "          alt.Y('Word:N', sort='-x')\n",
        "      )\n",
        "      return plot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuMxZvOBXZ6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "043f5ca9-cbab-4f2c-d927-8c2a1045466b"
      },
      "source": [
        "# Check word distribution w/o common stopwords\n",
        "additional_stopwords = [\"it\"]\n",
        "preprocessed_df = PreProcess(df, additional_stopwords)\n",
        "preprocessed_df.plot_word_distributions(50)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-1417a4cca8a344dc96b1f300f9198e02\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-1417a4cca8a344dc96b1f300f9198e02\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-1417a4cca8a344dc96b1f300f9198e02\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-41a2af9085492fd91a8547a0a527e2a2\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-41a2af9085492fd91a8547a0a527e2a2\": [{\"Word\": \"accused\", \"Count\": 23353}, {\"Word\": \"reporting\", \"Count\": 20264}, {\"Word\": \"party\", \"Count\": 20186}, {\"Word\": \"alleges\", \"Count\": 15661}, {\"Word\": \"officer\", \"Count\": 13874}, {\"Word\": \"alleged\", \"Count\": 10726}, {\"Word\": \"complainant\", \"Count\": 8905}, {\"Word\": \"officers\", \"Count\": 8192}, {\"Word\": \"victim\", \"Count\": 5716}, {\"Word\": \"vehicle\", \"Count\": 4842}, {\"Word\": \"failed\", \"Count\": 4751}, {\"Word\": \"police\", \"Count\": 4730}, {\"Word\": \"justification\", \"Count\": 3700}, {\"Word\": \"stated\", \"Count\": 3655}, {\"Word\": \"report\", \"Count\": 3213}, {\"Word\": \"male\", \"Count\": 2671}, {\"Word\": \"refused\", \"Count\": 2600}, {\"Word\": \"states\", \"Count\": 2510}, {\"Word\": \"called\", \"Count\": 2368}, {\"Word\": \"arrest\", \"Count\": 2306}, {\"Word\": \"did\", \"Count\": 2259}, {\"Word\": \"unknown\", \"Count\": 2145}, {\"Word\": \"searched\", \"Count\": 2055}, {\"Word\": \"told\", \"Count\": 2031}, {\"Word\": \"arrested\", \"Count\": 2031}, {\"Word\": \"rude\", \"Count\": 1898}, {\"Word\": \"citation\", \"Count\": 1793}, {\"Word\": \"stopped\", \"Count\": 1755}, {\"Word\": \"provide\", \"Count\": 1754}, {\"Word\": \"traffic\", \"Count\": 1718}, {\"Word\": \"incident\", \"Count\": 1630}, {\"Word\": \"regarding\", \"Count\": 1554}, {\"Word\": \"residence\", \"Count\": 1539}, {\"Word\": \"white\", \"Count\": 1526}, {\"Word\": \"unprofessional\", \"Count\": 1451}, {\"Word\": \"issued\", \"Count\": 1435}, {\"Word\": \"district\", \"Count\": 1380}, {\"Word\": \"threatened\", \"Count\": 1302}, {\"Word\": \"reported\", \"Count\": 1300}, {\"Word\": \"time\", \"Count\": 1296}, {\"Word\": \"car\", \"Count\": 1292}, {\"Word\": \"station\", \"Count\": 1279}, {\"Word\": \"uniformed\", \"Count\": 1189}, {\"Word\": \"black\", \"Count\": 1158}, {\"Word\": \"female\", \"Count\": 1134}, {\"Word\": \"witness\", \"Count\": 1116}, {\"Word\": \"case\", \"Count\": 1092}, {\"Word\": \"th\", \"Count\": 1087}, {\"Word\": \"number\", \"Count\": 1074}, {\"Word\": \"scene\", \"Count\": 1064}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQVPsaTEkxsF",
        "colab_type": "text"
      },
      "source": [
        "We want to look at categories like: nudity_penetration,sexual_relations_with_a_minor_,sexual_harassment_sexual_remarks,domestic_violence_police_committing_,sexual_humiliation_sexual_extortion_prostitution_sex_work,tasers_baton_aggressive_physical_touch_gun,trespass_robbery,biometric_surveillance_fitting_a_description_gang_related_,racial_slurs_xenophobic_remarks_,undocumented_status_asking_for_someone_s_status_calling_ice_,planting_drug_guns,neglect_of_duty_failure_to_serve,refusing_to_provide_medical_assistance,workplace_harassment,_irrational_aggressive_unstable_,suicide_in_jail_improper_care_,dcfs_threats,pregnant_women,school,searching_patting_down_arresting_minors\n",
        "\n",
        "So we will eliminate words like accused, reporting, party, alleges, officer, alleged, complainant, officers, victim, vehicle, failed, police, justification, stated, report, states, called, did, unknown, told, provide, incident, regarding, issued."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i113DtFKuv-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "outputId": "09833535-63eb-4209-b2a3-391f362d4f0f"
      },
      "source": [
        "# Remove common allegation jargon\n",
        "additional_stopwords = [\"accused\", \"reporting\", \"party\", \"alleges\", \"officer\", \n",
        "                        \"alleged\", \"complainant\", \"officers\", \"victim\", \n",
        "                        \"police\", \"stated\", \"report\", \"states\", \"called\", \n",
        "                        \"did\", \"told\", \"provide\", \"incident\", \"regarding\", \"issued\",\n",
        "                        \"reported\", \"vehicle\", \"car\", \"justification\",\n",
        "                        \"district\", \"uniformed\", \"threatened\", \"witness\", \"th\",\n",
        "                        \"number\", \"scene\"]\n",
        "preprocessed_df = PreProcess(df, additional_stopwords)\n",
        "preprocessed_df.plot_word_distributions(30)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-b2c499a7837f4da0ac8c1f833f2d57b3\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-b2c499a7837f4da0ac8c1f833f2d57b3\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-b2c499a7837f4da0ac8c1f833f2d57b3\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-f026144b7307db066b7739a9846547b0\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-f026144b7307db066b7739a9846547b0\": [{\"Word\": \"failed\", \"Count\": 4751}, {\"Word\": \"male\", \"Count\": 2671}, {\"Word\": \"refused\", \"Count\": 2600}, {\"Word\": \"arrest\", \"Count\": 2306}, {\"Word\": \"unknown\", \"Count\": 2145}, {\"Word\": \"searched\", \"Count\": 2055}, {\"Word\": \"arrested\", \"Count\": 2031}, {\"Word\": \"rude\", \"Count\": 1898}, {\"Word\": \"citation\", \"Count\": 1793}, {\"Word\": \"stopped\", \"Count\": 1755}, {\"Word\": \"traffic\", \"Count\": 1718}, {\"Word\": \"residence\", \"Count\": 1539}, {\"Word\": \"white\", \"Count\": 1526}, {\"Word\": \"unprofessional\", \"Count\": 1451}, {\"Word\": \"time\", \"Count\": 1296}, {\"Word\": \"station\", \"Count\": 1279}, {\"Word\": \"black\", \"Count\": 1158}, {\"Word\": \"female\", \"Count\": 1134}, {\"Word\": \"case\", \"Count\": 1092}, {\"Word\": \"responded\", \"Count\": 1058}, {\"Word\": \"falsely\", \"Count\": 1019}, {\"Word\": \"property\", \"Count\": 1013}, {\"Word\": \"stop\", \"Count\": 1004}, {\"Word\": \"son\", \"Count\": 998}, {\"Word\": \"information\", \"Count\": 998}, {\"Word\": \"return\", \"Count\": 991}, {\"Word\": \"phone\", \"Count\": 989}, {\"Word\": \"reason\", \"Count\": 981}, {\"Word\": \"warrant\", \"Count\": 965}, {\"Word\": \"false\", \"Count\": 959}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfPuB9nYoanp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreProcess():\n",
        "  '''\n",
        "  Class for pre-processing a csv of text documents into a  sparse matrix of\n",
        "  counts following scikit-learn's CountVectorizer\n",
        "\n",
        "  Source:\n",
        "  https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\n",
        "  '''\n",
        "\n",
        "  def __init__(self, raw_data, additional_stopwords, stem=True):\n",
        "      self.raw_data = raw_data\n",
        "      self.stemmer = PorterStemmer()\n",
        "      self.additional_stopwords = set(additional_stopwords)\n",
        "      self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "      self.stem = stem\n",
        "      self.vectorizer = None\n",
        "      self.fitted_vectorizer = None\n",
        "\n",
        "  def _add_stopwords(self):\n",
        "      '''\n",
        "      Add stopwords\n",
        "      '''\n",
        "      # combine english stopwords with own set\n",
        "      print(self.stop_words)\n",
        "      print(self.additional_stopwords)\n",
        "      self.stop_words = self.stop_words.union(self.additional_stopwords)\n",
        "\n",
        "\n",
        "  def _tokenize_text(self, text):\n",
        "      '''\n",
        "      Strips punctuation, and everything that isn't alphabetic, tokenizes.\n",
        "      Stems by default.\n",
        "      '''\n",
        "      # drop whatever isn't a word with letters or an apostrophe\n",
        "      if self.stem:\n",
        "        # stem\n",
        "        return [self.stemmer.stem(re.sub('[^a-z]', '', word.lower())) \n",
        "                for word in word_tokenize(text)\n",
        "                if word not in self.stop_words]\n",
        "      else:\n",
        "        # don't stem\n",
        "        return [re.sub('[^a-z]', '', word.lower())\n",
        "                for word in word_tokenize(text)\n",
        "                if word not in self.stop_words]\n",
        "\n",
        "\n",
        "  def _analyzer(self, text):\n",
        "      '''\n",
        "      Override analyzer. The analyzer calls _strip_text and self.stop_words\n",
        "      '''\n",
        "      # preprocess data before counts\n",
        "      return [word for word in self._strip_text(text)]\n",
        "  \n",
        "  def _vectorize(self):\n",
        "      '''\n",
        "      Launch a vectorizer with CountVectorizer\n",
        "      '''\n",
        "      # collect stopwords\n",
        "      self._add_stopwords()\n",
        "      # instantiate vectorizer w/ our custom analyzer\n",
        "      self.vectorizer = CountVectorizer(max_df=0.8, \n",
        "                                        min_df=2, \n",
        "                                        analyzer=self._analyzer)\n",
        "\n",
        "  def _fit_vectorizer(self):\n",
        "      '''\n",
        "      Fit vectorizer\n",
        "      '''\n",
        "      # launch the CountVectorizer object\n",
        "      self._vectorize()\n",
        "      # fit it\n",
        "      self.fitted_vectorizer = self.vectorizer.fit_transform(self.raw_data.text.values.astype('U'))\n",
        "\n",
        "\n",
        "  def plot_word_distributions(self, N):\n",
        "      '''\n",
        "      Plots frequencies for top N words\n",
        "\n",
        "      Source:\n",
        "        - https://altair-viz.github.io/gallery/percentage_of_total.html\n",
        "      '''\n",
        "      # get word list\n",
        "      if not self.fitted_vectorizer:\n",
        "        self._fit_vectorizer()\n",
        "\n",
        "      word_lst = self.vectorizer.get_feature_names()\n",
        "      counts_lst = np.asarray(self.fitted_vectorizer.sum(axis=0)).tolist()[0]\n",
        "\n",
        "      source = pd.DataFrame({'Word': word_lst, 'Count': counts_lst}).sort_values(by=['Count'], ascending=False)[:N]\n",
        "      \n",
        "      alt.data_transformers.disable_max_rows()\n",
        "\n",
        "      plot = alt.Chart(source).transform_joinaggregate(\n",
        "          TotalCount='sum(Count)',\n",
        "      ).transform_calculate(\n",
        "          PercentOfTotal=\"datum.Count / datum.TotalCount\"\n",
        "      ).mark_bar().encode(\n",
        "          alt.X('PercentOfTotal:Q', axis=alt.Axis(format='.0%')),\n",
        "          alt.Y('Word:N', sort='-x')\n",
        "      )\n",
        "      return plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pXQhb9fFxp2x",
        "colab": {}
      },
      "source": [
        "class PreProcess():\n",
        "  '''\n",
        "  Class for pre-processing a csv of text documents into a  sparse matrix of\n",
        "  counts following scikit-learn's CountVectorizer\n",
        "\n",
        "  Source:\n",
        "  https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\n",
        "  '''\n",
        "\n",
        "  def __init__(self, raw_data, additional_stopwords, stem=True):\n",
        "      self.raw_data = raw_data\n",
        "      self.stemmer = PorterStemmer()\n",
        "      self.stop_words = set(nltk.corpus.stopwords.words('english')).union(additional_stopwords)\n",
        "      self.stem = stem\n",
        "      self.vectorizer = None\n",
        "      self.fitted_vectorizer = None\n",
        "\n",
        "\n",
        "  def _tokenize_text(self, text):\n",
        "      '''\n",
        "      Strips punctuation, and everything that isn't alphabetic, tokenizes.\n",
        "      Stems by default.\n",
        "      '''\n",
        "      tokenized_text = []\n",
        "      \n",
        "      # drop whatever isn't a word with letters or an apostrophe\n",
        "      for token in word_tokenize(text):\n",
        "        # to lowercase\n",
        "        token = token.lower()\n",
        "        # substitute whatever is not alphabetic\n",
        "        token = re.sub('[^a-z]', '', token)\n",
        "        if token:\n",
        "          if token not in self.stop_words:\n",
        "            if self.stem:\n",
        "              tokenized_text.append(self.stemmer.stem(token))\n",
        "            else:\n",
        "              tokenized_text.append(token)\n",
        "      \n",
        "      return tokenized_text\n",
        "  \n",
        "  def _vectorize(self):\n",
        "      '''\n",
        "      Launch a vectorizer with CountVectorizer\n",
        "      '''\n",
        "      # instantiate vectorizer w/ our custom analyzer\n",
        "      # by default we drop words that appear in more than 80% of documents and\n",
        "      # that don't appear in more than one document\n",
        "      # we override the analyzer with our tokenizer method\n",
        "      self.vectorizer = CountVectorizer(max_df=0.8, \n",
        "                                        min_df=2, \n",
        "                                        analyzer=self._tokenize_text)\n",
        "\n",
        "  def _fit_vectorizer(self):\n",
        "      '''\n",
        "      Fit vectorizer\n",
        "      '''\n",
        "      # launch the CountVectorizer object\n",
        "      self._vectorize()\n",
        "      # fit it\n",
        "      self.fitted_vectorizer = self.vectorizer.fit_transform(self.raw_data.text.values.astype('U'))\n",
        "\n",
        "  def plot_word_distributions(self, N):\n",
        "      '''\n",
        "      Plots frequencies for top N words\n",
        "\n",
        "      Source:\n",
        "        - https://altair-viz.github.io/gallery/percentage_of_total.html\n",
        "      '''\n",
        "      # get word list\n",
        "      if not self.fitted_vectorizer:\n",
        "        self._fit_vectorizer()\n",
        "\n",
        "      word_lst = self.vectorizer.get_feature_names()\n",
        "      counts_lst = np.asarray(self.fitted_vectorizer.sum(axis=0)).tolist()[0]\n",
        "\n",
        "      source = pd.DataFrame({'Word': word_lst, 'Count': counts_lst}).sort_values(by=['Count'], ascending=False)[:N]\n",
        "      \n",
        "      alt.data_transformers.disable_max_rows()\n",
        "\n",
        "      plot = alt.Chart(source).transform_joinaggregate(\n",
        "          TotalCount='sum(Count)',\n",
        "      ).transform_calculate(\n",
        "          PercentOfTotal=\"datum.Count / datum.TotalCount\"\n",
        "      ).mark_bar().encode(\n",
        "          alt.X('PercentOfTotal:Q', axis=alt.Axis(format='.0%')),\n",
        "          alt.Y('Word:N', sort='-x')\n",
        "      )\n",
        "      return plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BMJkKYOHgfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "3b004ad9-0b24-4e35-e953-5adcd1c6c776"
      },
      "source": [
        "# with stemming\n",
        "additional_stopwords = set([\"accused\", \"reporting\", \"party\", \"alleges\", \"officer\", \n",
        "                        \"alleged\", \"alleges\", \"complainant\", \"officers\", \"victim\", \n",
        "                        \"police\", \"stated\", \"report\", \"states\", \"called\", \n",
        "                        \"did\", \"told\", \"provide\", \"incident\", \"regarding\", \"issued\",\n",
        "                        \"reported\", \"vehicle\", \"car\", \"justification\",\n",
        "                        \"district\", \"uniformed\", \"threatened\", \"witness\", \"th\",\n",
        "                        \"number\", \"scene\"]).union(text.ENGLISH_STOP_WORDS)\n",
        "preprocessed_df = PreProcess(df, additional_stopwords, True)\n",
        "preprocessed_df.plot_word_distributions(20)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-04c4668c8b07409ea0288996b0b36577\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-04c4668c8b07409ea0288996b0b36577\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-04c4668c8b07409ea0288996b0b36577\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-155c57d7bead33861f4d9a6d46334d0f\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-155c57d7bead33861f4d9a6d46334d0f\": [{\"Word\": \"fail\", \"Count\": 4810}, {\"Word\": \"arrest\", \"Count\": 4501}, {\"Word\": \"search\", \"Count\": 2930}, {\"Word\": \"stop\", \"Count\": 2869}, {\"Word\": \"refus\", \"Count\": 2682}, {\"Word\": \"male\", \"Count\": 2580}, {\"Word\": \"citat\", \"Count\": 2547}, {\"Word\": \"unknown\", \"Count\": 2145}, {\"Word\": \"fals\", \"Count\": 1974}, {\"Word\": \"rude\", \"Count\": 1940}, {\"Word\": \"time\", \"Count\": 1929}, {\"Word\": \"inform\", \"Count\": 1751}, {\"Word\": \"resid\", \"Count\": 1745}, {\"Word\": \"respond\", \"Count\": 1723}, {\"Word\": \"traffic\", \"Count\": 1716}, {\"Word\": \"white\", \"Count\": 1505}, {\"Word\": \"unprofession\", \"Count\": 1467}, {\"Word\": \"fuck\", \"Count\": 1437}, {\"Word\": \"return\", \"Count\": 1308}, {\"Word\": \"harass\", \"Count\": 1284}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GResRMhPvQwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f8ab1988-7065-4aff-fe19-75e5ca37968e"
      },
      "source": [
        "# without stemming\n",
        "additional_stopwords = set([\"accused\", \"reporting\", \"party\", \"alleges\", \"officer\", \n",
        "                        \"alleged\", \"alleges\", \"complainant\", \"officers\", \"victim\", \n",
        "                        \"police\", \"stated\", \"report\", \"states\", \"called\", \n",
        "                        \"did\", \"told\", \"provide\", \"incident\", \"regarding\", \"issued\",\n",
        "                        \"reported\", \"vehicle\", \"car\", \"justification\",\n",
        "                        \"district\", \"uniformed\", \"threatened\", \"witness\", \"th\",\n",
        "                        \"number\", \"scene\"]).union(text.ENGLISH_STOP_WORDS)\n",
        "preprocessed_df = PreProcess(df, additional_stopwords, False)\n",
        "preprocessed_df.plot_word_distributions(50)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-98bb2cb3ee3349c6ad88ba33552cb371\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-98bb2cb3ee3349c6ad88ba33552cb371\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-98bb2cb3ee3349c6ad88ba33552cb371\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-707d58b41817de738d9c33417e4815be\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-707d58b41817de738d9c33417e4815be\": [{\"Word\": \"failed\", \"Count\": 4751}, {\"Word\": \"refused\", \"Count\": 2600}, {\"Word\": \"male\", \"Count\": 2502}, {\"Word\": \"arrest\", \"Count\": 2305}, {\"Word\": \"unknown\", \"Count\": 2145}, {\"Word\": \"searched\", \"Count\": 2053}, {\"Word\": \"arrested\", \"Count\": 2029}, {\"Word\": \"rude\", \"Count\": 1898}, {\"Word\": \"citation\", \"Count\": 1791}, {\"Word\": \"stopped\", \"Count\": 1755}, {\"Word\": \"traffic\", \"Count\": 1716}, {\"Word\": \"residence\", \"Count\": 1538}, {\"Word\": \"white\", \"Count\": 1505}, {\"Word\": \"unprofessional\", \"Count\": 1451}, {\"Word\": \"time\", \"Count\": 1292}, {\"Word\": \"station\", \"Count\": 1274}, {\"Word\": \"black\", \"Count\": 1130}, {\"Word\": \"female\", \"Count\": 1114}, {\"Word\": \"case\", \"Count\": 1091}, {\"Word\": \"responded\", \"Count\": 1058}, {\"Word\": \"falsely\", \"Count\": 1019}, {\"Word\": \"property\", \"Count\": 1011}, {\"Word\": \"stop\", \"Count\": 998}, {\"Word\": \"information\", \"Count\": 998}, {\"Word\": \"return\", \"Count\": 991}, {\"Word\": \"reason\", \"Count\": 979}, {\"Word\": \"phone\", \"Count\": 967}, {\"Word\": \"warrant\", \"Count\": 962}, {\"Word\": \"false\", \"Count\": 955}, {\"Word\": \"son\", \"Count\": 930}, {\"Word\": \"license\", \"Count\": 913}, {\"Word\": \"fuck\", \"Count\": 879}, {\"Word\": \"asked\", \"Count\": 879}, {\"Word\": \"nt\", \"Count\": 879}, {\"Word\": \"offender\", \"Count\": 859}, {\"Word\": \"door\", \"Count\": 853}, {\"Word\": \"entered\", \"Count\": 841}, {\"Word\": \"home\", \"Count\": 840}, {\"Word\": \"inventory\", \"Count\": 839}, {\"Word\": \"going\", \"Count\": 812}, {\"Word\": \"accident\", \"Count\": 790}, {\"Word\": \"file\", \"Count\": 764}, {\"Word\": \"citations\", \"Count\": 756}, {\"Word\": \"search\", \"Count\": 750}, {\"Word\": \"sergeant\", \"Count\": 739}, {\"Word\": \"went\", \"Count\": 721}, {\"Word\": \"supervisor\", \"Count\": 714}, {\"Word\": \"driver\", \"Count\": 713}, {\"Word\": \"partyvictim\", \"Count\": 706}, {\"Word\": \"subject\", \"Count\": 702}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuvhVqxz3lSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d17d7c61-47a8-4839-a474-3fe8c37211ca"
      },
      "source": [
        "additional_stopwords"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'accused',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'alleged',\n",
              " 'alleges',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amoungst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anyhow',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'are',\n",
              " 'around',\n",
              " 'as',\n",
              " 'at',\n",
              " 'back',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'bill',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'but',\n",
              " 'by',\n",
              " 'call',\n",
              " 'called',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'cant',\n",
              " 'car',\n",
              " 'co',\n",
              " 'complainant',\n",
              " 'con',\n",
              " 'could',\n",
              " 'couldnt',\n",
              " 'cry',\n",
              " 'de',\n",
              " 'describe',\n",
              " 'detail',\n",
              " 'did',\n",
              " 'district',\n",
              " 'do',\n",
              " 'done',\n",
              " 'down',\n",
              " 'due',\n",
              " 'during',\n",
              " 'each',\n",
              " 'eg',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'enough',\n",
              " 'etc',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'except',\n",
              " 'few',\n",
              " 'fifteen',\n",
              " 'fifty',\n",
              " 'fill',\n",
              " 'find',\n",
              " 'fire',\n",
              " 'first',\n",
              " 'five',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forty',\n",
              " 'found',\n",
              " 'four',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'further',\n",
              " 'get',\n",
              " 'give',\n",
              " 'go',\n",
              " 'had',\n",
              " 'has',\n",
              " 'hasnt',\n",
              " 'have',\n",
              " 'he',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'however',\n",
              " 'hundred',\n",
              " 'i',\n",
              " 'ie',\n",
              " 'if',\n",
              " 'in',\n",
              " 'inc',\n",
              " 'incident',\n",
              " 'indeed',\n",
              " 'interest',\n",
              " 'into',\n",
              " 'is',\n",
              " 'issued',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'justification',\n",
              " 'keep',\n",
              " 'last',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'least',\n",
              " 'less',\n",
              " 'ltd',\n",
              " 'made',\n",
              " 'many',\n",
              " 'may',\n",
              " 'me',\n",
              " 'meanwhile',\n",
              " 'might',\n",
              " 'mill',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'much',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'name',\n",
              " 'namely',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'next',\n",
              " 'nine',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'none',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'number',\n",
              " 'of',\n",
              " 'off',\n",
              " 'officer',\n",
              " 'officers',\n",
              " 'often',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'or',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 'part',\n",
              " 'party',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'please',\n",
              " 'police',\n",
              " 'provide',\n",
              " 'put',\n",
              " 'rather',\n",
              " 're',\n",
              " 'regarding',\n",
              " 'report',\n",
              " 'reported',\n",
              " 'reporting',\n",
              " 'same',\n",
              " 'scene',\n",
              " 'see',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'serious',\n",
              " 'several',\n",
              " 'she',\n",
              " 'should',\n",
              " 'show',\n",
              " 'side',\n",
              " 'since',\n",
              " 'sincere',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhere',\n",
              " 'stated',\n",
              " 'states',\n",
              " 'still',\n",
              " 'such',\n",
              " 'system',\n",
              " 'take',\n",
              " 'ten',\n",
              " 'th',\n",
              " 'than',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'thereupon',\n",
              " 'these',\n",
              " 'they',\n",
              " 'thick',\n",
              " 'thin',\n",
              " 'third',\n",
              " 'this',\n",
              " 'those',\n",
              " 'though',\n",
              " 'threatened',\n",
              " 'three',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'to',\n",
              " 'together',\n",
              " 'told',\n",
              " 'too',\n",
              " 'top',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'twelve',\n",
              " 'twenty',\n",
              " 'two',\n",
              " 'un',\n",
              " 'under',\n",
              " 'uniformed',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'us',\n",
              " 'vehicle',\n",
              " 'very',\n",
              " 'via',\n",
              " 'victim',\n",
              " 'was',\n",
              " 'we',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'whatever',\n",
              " 'when',\n",
              " 'whence',\n",
              " 'whenever',\n",
              " 'where',\n",
              " 'whereafter',\n",
              " 'whereas',\n",
              " 'whereby',\n",
              " 'wherein',\n",
              " 'whereupon',\n",
              " 'wherever',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'whither',\n",
              " 'who',\n",
              " 'whoever',\n",
              " 'whole',\n",
              " 'whom',\n",
              " 'whose',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'witness',\n",
              " 'would',\n",
              " 'yet',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    }
  ]
}