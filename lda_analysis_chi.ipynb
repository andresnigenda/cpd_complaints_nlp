{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of lda_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG7Wsr5JR_ZW",
        "colab_type": "text"
      },
      "source": [
        "# **LDA of complaints against the CPD (Chi version)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZKVIPm4RgZq",
        "colab_type": "text"
      },
      "source": [
        "### *Importing Packages*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdsgV4neHweG",
        "colab_type": "code",
        "outputId": "b2b5830e-7012-43d2-8878-84be8c045eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction import text \n",
        "from spacy.tokenizer import Tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from spacy.lang.en import English\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "from nltk import word_tokenize\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffM_7r46SvlL",
        "colab_type": "text"
      },
      "source": [
        "### *Pre-processing*\n",
        "0. Set up allegations data from csv\n",
        "1. Convert data to lowercase\n",
        "2. Remove special characters (punctuation and numbers)\n",
        "3. Tokenize into terms\n",
        "4. Remove stop words (generic + allegation specific)\n",
        "5. Stemming\n",
        "6. Term document matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbUzq1roSvBU",
        "colab_type": "code",
        "outputId": "5a36cd38-912d-4dfa-9945-6a65a93d6115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "source": [
        "# 0. Set up allegations data from csv #\n",
        "#\n",
        "# read in csv\n",
        "narratives_csv_url = \"https://raw.githubusercontent.com/andresnigenda/cpd_complaints_nlp/andres/narratives.csv\"\n",
        "df = pd.read_csv(narratives_csv_url)\n",
        "# filter to relevant section\n",
        "df = df[df.column_name == \"Initial / Intake Allegation\"]\n",
        "\n",
        "#Check if we duplicates are due to different variation in columns other than cr_id and text\n",
        "df.drop(columns=['pdf_name','doccloud_url','dropbox_path','page_num'], inplace=True)\n",
        "filtered_df = df.drop_duplicates().reset_index()\n",
        "print(\"There are {} complaints after dropping columns\".format(filtered_df.shape[0]))\n",
        "print(filtered_df)\n",
        "#Looks like most of the duplicates come from the other columns so we want to keep rows with unique content for cr_id and text\n",
        "# filter to relevant columns\n",
        "df = df[['cr_id', 'text']]\n",
        "print(\"There are {} complaints\".format(df.shape[0]))\n",
        "# drop allegations with same id + text\n",
        "df = df.drop_duplicates(['cr_id', 'text'])\n",
        "print(\"There are {} unique complaints\".format(df.shape[0]))\n",
        "#allegations_lst = df['text'].to_list()\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 17059 complaints after dropping columns\n",
            "       index  ...  batch_id\n",
            "0          0  ...         1\n",
            "1          4  ...         1\n",
            "2          9  ...         1\n",
            "3         12  ...         1\n",
            "4         14  ...         1\n",
            "...      ...  ...       ...\n",
            "17054  30703  ...         5\n",
            "17055  30706  ...         5\n",
            "17056  30707  ...         5\n",
            "17057  30711  ...         5\n",
            "17058  30715  ...         5\n",
            "\n",
            "[17059 rows x 6 columns]\n",
            "There are 19966 complaints\n",
            "There are 17001 unique complaints\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cr_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1048960</td>\n",
              "      <td>The reporting party alleges that the\\naccused ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1048962</td>\n",
              "      <td>The victim alleges that an unknown male\\nblack...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1048964</td>\n",
              "      <td>The reporting party alleges that he was a\\nvi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1048965</td>\n",
              "      <td>The reporting party alleges that while\\nwaitin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1048965</td>\n",
              "      <td>The reporting party alleges that while\\nwaitin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cr_id                                               text\n",
              "0   1048960  The reporting party alleges that the\\naccused ...\n",
              "4   1048962  The victim alleges that an unknown male\\nblack...\n",
              "9   1048964   The reporting party alleges that he was a\\nvi...\n",
              "12  1048965  The reporting party alleges that while\\nwaitin...\n",
              "14  1048965  The reporting party alleges that while\\nwaitin..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP7xNH6Q0iLT",
        "colab_type": "code",
        "outputId": "7fc417dc-cdac-4ab0-d21e-5ae7d897311a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#The text columns from some rows contain duplicated content.\n",
        "#The functions below will help detect them.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cosine_sim_vectors(vec1, vec2):\n",
        "  vec1 = vec1.reshape(1,-1)\n",
        "  vec2 = vec2.reshape(1,-1)\n",
        "  return cosine_similarity(vec1, vec2)[0][0]\n",
        "\n",
        "def split_half_and_compare(str_input):\n",
        "  if len(str_input) < 2 or str_input in ['nfi', 'NFI'] or str_input in nltk.corpus.stopwords.words('english'):\n",
        "    return False\n",
        "  first_half = str_input[0:len(str_input)//2] \n",
        "  second_half = str_input[len(str_input)//2 if len(str_input)%2 == 0\n",
        "                                 else ((len(str_input)//2)+1):]\n",
        "  #print(first_half)\n",
        "  #print(second_half)\n",
        "  vectorizer = CountVectorizer().fit_transform([first_half, second_half])\n",
        "  vectors = vectorizer.toarray()\n",
        "  similarity_score = cosine_sim_vectors(vectors[0], vectors[1])\n",
        "  #print(similarity_score)\n",
        "  return similarity_score > .9\n",
        "\n",
        "test_str = filtered_df.iloc[17055]['text']\n",
        "split_half_and_compare(test_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdNUVfaP69Gx",
        "colab_type": "code",
        "outputId": "25edaae8-1de8-4f9a-c1e6-5a9574fd6035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df['TextCount'] = df['text'].apply(len)\n",
        "df['DuplicateText'] = df['text'].apply(split_half_and_compare)\n",
        "duplicated_df = df[df['DuplicateText'] == True].reset_index()\n",
        "#randomly grab one row to check\n",
        "print(duplicated_df.iloc[4]['text'])\n",
        "print(\"\")\n",
        "print(\"Threre are {} rows with duplicated content issue\".format(duplicated_df.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The reporting party alleges that the accused\n",
            "officers failed to inventory or return his\n",
            "identification\n",
            "The reporting party alleges that the accused\n",
            "officers failed to inventory or return his\n",
            "identification\n",
            "\n",
            "Threre are 726 rows with duplicated content issue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH8Ej9nkA-vS",
        "colab_type": "code",
        "outputId": "cbf50526-5b0d-4e27-adff-e8a8b4e6dbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#If DuplicateText is True then replace text by half of text\n",
        "df['text'] = df.apply(lambda row: row['text'] if row['DuplicateText'] == False else row['text'][0:len(row['text'])//2], axis=1)\n",
        "#double check if processing works\n",
        "df[df['DuplicateText'] == True].reset_index().iloc[4]['text']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The reporting party alleges that the accused\\nofficers failed to inventory or return his\\nidentification'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQVPsaTEkxsF",
        "colab_type": "text"
      },
      "source": [
        "We want to look at categories like: nudity_penetration,sexual_relations_with_a_minor_,sexual_harassment_sexual_remarks,domestic_violence_police_committing_,sexual_humiliation_sexual_extortion_prostitution_sex_work,tasers_baton_aggressive_physical_touch_gun,trespass_robbery,biometric_surveillance_fitting_a_description_gang_related_,racial_slurs_xenophobic_remarks_,undocumented_status_asking_for_someone_s_status_calling_ice_,planting_drug_guns,neglect_of_duty_failure_to_serve,refusing_to_provide_medical_assistance,workplace_harassment,_irrational_aggressive_unstable_,suicide_in_jail_improper_care_,dcfs_threats,pregnant_women,school,searching_patting_down_arresting_minors\n",
        "\n",
        "So we will eliminate words like accused, reporting, party, alleges, officer, alleged, complainant, officers, victim, vehicle, failed, police, justification, stated, report, states, called, did, unknown, told, provide, incident, regarding, issued."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pXQhb9fFxp2x",
        "colab": {}
      },
      "source": [
        "class PreProcess():\n",
        "  '''\n",
        "  Class for pre-processing a csv of text documents into a  sparse matrix of\n",
        "  counts following scikit-learn's CountVectorizer\n",
        "\n",
        "  Source:\n",
        "  https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\n",
        "  '''\n",
        "\n",
        "  def __init__(self, raw_data, additional_stopwords, stem=True):\n",
        "      self.raw_data = raw_data\n",
        "      self.stemmer = PorterStemmer()\n",
        "      self.stop_words = set(nltk.corpus.stopwords.words('english')).union(additional_stopwords)\n",
        "      self.stem = stem\n",
        "      self.vectorizer = None\n",
        "      self.doc_term_matrix = None\n",
        "\n",
        "\n",
        "  def _tokenize_text(self, text):\n",
        "      '''\n",
        "      Strips punctuation, and everything that isn't alphabetic, tokenizes.\n",
        "      Stems by default.\n",
        "      '''\n",
        "      tokenized_text = []\n",
        "      \n",
        "      # drop whatever isn't a word with letters or an apostrophe\n",
        "      for token in word_tokenize(text):\n",
        "        # to lowercase\n",
        "        token = token.lower()\n",
        "        # substitute whatever is not alphabetic\n",
        "        token = re.sub('[^a-z]', '', token)\n",
        "        if token:\n",
        "          if token not in self.stop_words:\n",
        "            if self.stem:\n",
        "              tokenized_text.append(self.stemmer.stem(token))\n",
        "            else:\n",
        "              tokenized_text.append(token)\n",
        "      \n",
        "      return tokenized_text\n",
        "  \n",
        "  def _vectorize(self):\n",
        "      '''\n",
        "      Launch a vectorizer with CountVectorizer\n",
        "      '''\n",
        "      # instantiate vectorizer w/ our custom analyzer\n",
        "      # by default we drop words that appear in more than 80% of documents and\n",
        "      # that don't appear in more than one document\n",
        "      # we override the analyzer with our tokenizer method\n",
        "      self.vectorizer = CountVectorizer(max_df=0.8, \n",
        "                                        min_df=2, \n",
        "                                        analyzer=self._tokenize_text)\n",
        "\n",
        "  def _fit_vectorizer(self):\n",
        "      '''\n",
        "      Fit vectorizer and create a doc_term_matrix\n",
        "      '''\n",
        "      # launch the CountVectorizer object\n",
        "      self._vectorize()\n",
        "      # fit it\n",
        "      self.doc_term_matrix = self.vectorizer.fit_transform(self.raw_data.text.values.astype('U'))\n",
        "\n",
        "  def plot_word_distributions(self, N):\n",
        "      '''\n",
        "      Plots frequencies for top N words\n",
        "\n",
        "      Source:\n",
        "        - https://altair-viz.github.io/gallery/percentage_of_total.html\n",
        "      '''\n",
        "      # get word list\n",
        "      if not self.doc_term_matrix:\n",
        "        self._fit_vectorizer()\n",
        "\n",
        "      word_lst = self.vectorizer.get_feature_names()\n",
        "      counts_lst = np.asarray(self.doc_term_matrix.sum(axis=0)).tolist()[0]\n",
        "\n",
        "      source = pd.DataFrame({'Word': word_lst, 'Count': counts_lst}).sort_values(by=['Count'], ascending=False)[:N]\n",
        "      \n",
        "      alt.data_transformers.disable_max_rows()\n",
        "\n",
        "      plot = alt.Chart(source).transform_joinaggregate(\n",
        "          TotalCount='sum(Count)',\n",
        "      ).transform_calculate(\n",
        "          PercentOfTotal=\"datum.Count / datum.TotalCount\"\n",
        "      ).mark_bar().encode(\n",
        "          alt.X('PercentOfTotal:Q', axis=alt.Axis(format='.0%')),\n",
        "          alt.Y('Word:N', sort='-x')\n",
        "      )\n",
        "      return plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaVwXBAtDpae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "additional_stopwords = set([\"accused\", \"reporting\", \"party\", \"alleges\", \"officer\", \n",
        "                        \"alleged\", \"alleges\", \"complainant\", \"officers\", \"victim\", \n",
        "                        \"police\", \"stated\", \"report\", \"states\", \"called\", \n",
        "                        \"did\", \"told\", \"provide\", \"incident\", \"regarding\", \"issued\",\n",
        "                        \"reported\", \"vehicle\", \"car\", \"justification\",\n",
        "                        \"district\", \"uniformed\", \"threatened\", \"witness\", \"th\",\n",
        "                        \"number\", \"scene\"]).union(text.ENGLISH_STOP_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BMJkKYOHgfT",
        "colab_type": "code",
        "outputId": "bcfb7c70-f6cd-4fff-f9ea-124676f56cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "# with stemming\n",
        "stemmed_data = PreProcess(df, additional_stopwords, True)\n",
        "stemmed_data.plot_word_distributions(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-98702925221f4f05b31ad5b7d086f5a2\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-98702925221f4f05b31ad5b7d086f5a2\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-98702925221f4f05b31ad5b7d086f5a2\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0a5a6f651715fc2be69f19dbe5c67b53\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-0a5a6f651715fc2be69f19dbe5c67b53\": [{\"Word\": \"fail\", \"Count\": 4523}, {\"Word\": \"arrest\", \"Count\": 4081}, {\"Word\": \"stop\", \"Count\": 2730}, {\"Word\": \"search\", \"Count\": 2719}, {\"Word\": \"refus\", \"Count\": 2582}, {\"Word\": \"male\", \"Count\": 2558}, {\"Word\": \"citat\", \"Count\": 2497}, {\"Word\": \"unknown\", \"Count\": 2131}, {\"Word\": \"rude\", \"Count\": 1887}, {\"Word\": \"time\", \"Count\": 1857}, {\"Word\": \"fals\", \"Count\": 1762}, {\"Word\": \"inform\", \"Count\": 1708}, {\"Word\": \"respond\", \"Count\": 1665}, {\"Word\": \"traffic\", \"Count\": 1660}, {\"Word\": \"resid\", \"Count\": 1649}, {\"Word\": \"white\", \"Count\": 1488}, {\"Word\": \"unprofession\", \"Count\": 1425}, {\"Word\": \"fuck\", \"Count\": 1400}, {\"Word\": \"harass\", \"Count\": 1258}, {\"Word\": \"return\", \"Count\": 1217}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GResRMhPvQwH",
        "colab_type": "code",
        "outputId": "977f260a-dc8c-452e-dba6-376c5bf289f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# without stemming\n",
        "non_stemmed_data = PreProcess(df, additional_stopwords, False)\n",
        "non_stemmed_data.plot_word_distributions(50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-fd5845b4143147b7bcb6fc1f2da20cd2\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-fd5845b4143147b7bcb6fc1f2da20cd2\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-fd5845b4143147b7bcb6fc1f2da20cd2\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0bbddf858a5c9a9c7f987a243f964c0e\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"axis\": {\"format\": \".0%\"}, \"field\": \"PercentOfTotal\"}, \"y\": {\"type\": \"nominal\", \"field\": \"Word\", \"sort\": \"-x\"}}, \"transform\": [{\"joinaggregate\": [{\"op\": \"sum\", \"field\": \"Count\", \"as\": \"TotalCount\"}]}, {\"calculate\": \"datum.Count / datum.TotalCount\", \"as\": \"PercentOfTotal\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-0bbddf858a5c9a9c7f987a243f964c0e\": [{\"Word\": \"failed\", \"Count\": 4464}, {\"Word\": \"refused\", \"Count\": 2503}, {\"Word\": \"male\", \"Count\": 2480}, {\"Word\": \"arrest\", \"Count\": 2135}, {\"Word\": \"unknown\", \"Count\": 2131}, {\"Word\": \"searched\", \"Count\": 1915}, {\"Word\": \"rude\", \"Count\": 1849}, {\"Word\": \"arrested\", \"Count\": 1802}, {\"Word\": \"citation\", \"Count\": 1764}, {\"Word\": \"stopped\", \"Count\": 1660}, {\"Word\": \"traffic\", \"Count\": 1660}, {\"Word\": \"white\", \"Count\": 1488}, {\"Word\": \"residence\", \"Count\": 1444}, {\"Word\": \"unprofessional\", \"Count\": 1409}, {\"Word\": \"time\", \"Count\": 1249}, {\"Word\": \"station\", \"Count\": 1203}, {\"Word\": \"black\", \"Count\": 1104}, {\"Word\": \"female\", \"Count\": 1096}, {\"Word\": \"case\", \"Count\": 1043}, {\"Word\": \"responded\", \"Count\": 1015}, {\"Word\": \"information\", \"Count\": 978}, {\"Word\": \"stop\", \"Count\": 956}, {\"Word\": \"phone\", \"Count\": 936}, {\"Word\": \"reason\", \"Count\": 936}, {\"Word\": \"return\", \"Count\": 915}, {\"Word\": \"son\", \"Count\": 907}, {\"Word\": \"property\", \"Count\": 897}, {\"Word\": \"false\", \"Count\": 893}, {\"Word\": \"license\", \"Count\": 874}, {\"Word\": \"falsely\", \"Count\": 869}, {\"Word\": \"nt\", \"Count\": 867}, {\"Word\": \"warrant\", \"Count\": 865}, {\"Word\": \"asked\", \"Count\": 856}, {\"Word\": \"fuck\", \"Count\": 855}, {\"Word\": \"home\", \"Count\": 830}, {\"Word\": \"door\", \"Count\": 818}, {\"Word\": \"going\", \"Count\": 800}, {\"Word\": \"offender\", \"Count\": 794}, {\"Word\": \"entered\", \"Count\": 789}, {\"Word\": \"inventory\", \"Count\": 763}, {\"Word\": \"accident\", \"Count\": 751}, {\"Word\": \"file\", \"Count\": 744}, {\"Word\": \"citations\", \"Count\": 733}, {\"Word\": \"sergeant\", \"Count\": 731}, {\"Word\": \"went\", \"Count\": 700}, {\"Word\": \"supervisor\", \"Count\": 695}, {\"Word\": \"partyvictim\", \"Count\": 684}, {\"Word\": \"subject\", \"Count\": 682}, {\"Word\": \"search\", \"Count\": 682}, {\"Word\": \"driver\", \"Count\": 682}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62IVHXpbC2n3",
        "colab_type": "text"
      },
      "source": [
        "### *Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZz_UFGC6OV",
        "colab_type": "code",
        "outputId": "4b37418c-1aa8-4946-b25f-8306a366aca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "LDA = LatentDirichletAllocation(n_components=20, random_state=8)\n",
        "LDA.fit(stemmed_data.doc_term_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=20, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=8, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TdqpEXBEQJE",
        "colab_type": "code",
        "outputId": "654b11c9-9a99-4cc9-afb0-0b4a3cf470d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i, topic in enumerate(LDA.components_):\n",
        "    print(f'Top 10 words for topic {i + 1}:')\n",
        "    print([stemmed_data.vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words for topic 1:\n",
            "['refus', 'place', 'allow', 'alleg', 'probabl', 'victim', 'search', 'fals', 'caus', 'arrest']\n",
            "\n",
            "\n",
            "Top 10 words for topic 2:\n",
            "['answer', 'inform', 'refus', 'unprofession', 'supervisor', 'phone', 'hung', 'rude', 'speak', 'telephon']\n",
            "\n",
            "\n",
            "Top 10 words for topic 3:\n",
            "['person', 'secur', 'properti', 'stolen', 'case', 'thorough', 'inform', 'conduct', 'investig', 'fail']\n",
            "\n",
            "\n",
            "Top 10 words for topic 4:\n",
            "['took', 'cell', 'properti', 'card', 'remov', 'usc', 'phone', 'fail', 'inventori', 'return']\n",
            "\n",
            "\n",
            "Top 10 words for topic 5:\n",
            "['unknown', 'home', 'damag', 'permiss', 'apart', 'warrant', 'door', 'enter', 'search', 'resid']\n",
            "\n",
            "\n",
            "Top 10 words for topic 6:\n",
            "['plant', 'drug', 'possess', 'assault', 'refus', 'batteri', 'fail', 'fals', 'offend', 'arrest']\n",
            "\n",
            "\n",
            "Top 10 words for topic 7:\n",
            "['possibl', 'supervisor', 'gun', 'process', 'unknown', 'white', 'femal', 'black', 'subject', 'male']\n",
            "\n",
            "\n",
            "Top 10 words for topic 8:\n",
            "['appar', 'reason', 'femal', 'plaincloth', 'hispan', 'unknown', 'possibl', 'black', 'white', 'male']\n",
            "\n",
            "\n",
            "Top 10 words for topic 9:\n",
            "['complaint', 'unknown', 'badg', 'taser', 'number', 'sergeant', 'name', 'refus', 'request', 'star']\n",
            "\n",
            "\n",
            "Top 10 words for topic 10:\n",
            "['abus', 'time', 'verbal', 'point', 'sergeant', 'hour', 'approxim', 'weapon', 'unknown', 'harass']\n",
            "\n",
            "\n",
            "Top 10 words for topic 11:\n",
            "['struck', 'drove', 'light', 'drive', 'nt', 'manner', 'fuck', 'yell', 'unprofession', 'rude']\n",
            "\n",
            "\n",
            "Top 10 words for topic 12:\n",
            "['inappropri', 'boyfriend', 'comment', 'squad', 'time', 'ask', 'quot', 'search', 'harass', 'stop']\n",
            "\n",
            "\n",
            "Top 10 words for topic 13:\n",
            "['inform', 'ticket', 'stop', 'plate', 'fals', 'driver', 'licens', 'accid', 'traffic', 'citat']\n",
            "\n",
            "\n",
            "Top 10 words for topic 14:\n",
            "['date', 'offend', 'father', 'child', 'case', 'detect', 'time', 'servic', 'respond', 'fail']\n",
            "\n",
            "\n",
            "Top 10 words for topic 15:\n",
            "['pull', 'fals', 'traffic', 'driver', 'handcuf', 'detain', 'citat', 'reason', 'search', 'stop']\n",
            "\n",
            "\n",
            "Top 10 words for topic 16:\n",
            "['unknown', 'cpd', 'tow', 'duti', 'hospit', 'chicago', 'activ', 'drug', 'member', 'depart']\n",
            "\n",
            "\n",
            "Top 10 words for topic 17:\n",
            "['block', 'verbal', 'time', 'disput', 'ticket', 'respond', 'citat', 'alterc', 'involv', 'park']\n",
            "\n",
            "\n",
            "Top 10 words for topic 18:\n",
            "['store', 'nt', 'refus', 'hospit', 'unknown', 'transport', 'time', 'year', 'old', 'son']\n",
            "\n",
            "\n",
            "Top 10 words for topic 19:\n",
            "['chicago', 'ass', 'shut', 'said', 'refer', 'abus', 'bitch', 'verbal', 'partyvictim', 'fuck']\n",
            "\n",
            "\n",
            "Top 10 words for topic 20:\n",
            "['case', 'went', 'inform', 'respond', 'assist', 'complaint', 'fail', 'station', 'file', 'refus']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o8nnuJjGOgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}