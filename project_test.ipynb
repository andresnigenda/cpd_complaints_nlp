{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgzsCnlliNXS"
   },
   "source": [
    "# Homework 1 Problem 4\n",
    "\n",
    "In this programming assignment, you will work on sentiment analysis using logistic regression with bag of words.  To help you quickly get started much of the required code has already been provided.  You primary task is to understand the provided code and fill in the gaps.  In particular, explicit preprocessing code has been provided so help you understand it clearly.  Gaps that you have to fill are marked with \"##YOUR CODE HERE ##\".\n",
    "\n",
    "This assignment will get you started with PyTorch.  We strongly recommend reading the first two tutorials on \"Deep Learning for NLP with PyTorch\" by Robert Guthrie https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html before you begin this assignment.  \n",
    "\n",
    "You should also seek our help unhesitatingly.  We want you to learn a reasonable amount of material in a short period of time, and our help will make it easier.  Please post your questions on Piazza or meet with us during office hours.\n",
    "\n",
    "You will work on part of the large movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/). The task is to classify movie reviews into two categories, POSITIVE or NEGATIVE. \n",
    "\n",
    "You are provided with a training set (TRAIN), a development set (DEV), and a test set (TEST). Your classifier is trained on TRAIN, evaluated and tuned on DEV, and tested on TEST.  It is best to work with a small sample training set while you are developing the code, but you should report your results on the full training set.\n",
    "\n",
    "OPTIONAL EXTENSIONS (Do not submit for grading):\n",
    "- Try to improve the model's performance by tuning the hyper parameters such as number of epochs, batch size, learning rate, choice of optimizer, etc. Also try to add regularization to the loss function.\n",
    "- The provided code takes advantage of a GPU (via cuda) if one is available.  If you find that your machine takes 10-15 minutes for one epoch of training, you can optionally run your code on Google collab.  Go to https://colab.research.google.com and log in using your Google account.  To upload a python notebook, click on \"Files\" dropdown menu and the upload notebook. To use GPU click the “Runtime” dropdown menu. Select “Change runtime type”. Select Python3 from “Runtime type” dropdown menu and choose hardware accelerator as GPU. You can find further instructions on how to use google collab in, e.g., the following pages: https://www.geeksforgeeks.org/how-to-use-google-colab/ and https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVQnA3_6iNXc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "def word_tokenize(s):\n",
    "    return s.split()\n",
    "\n",
    "# set the random seeds so the experiments can be replicated exactly\n",
    "random.seed(30255)\n",
    "np.random.seed(30255)\n",
    "torch.manual_seed(30255)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(30255)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"training_with_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_allegation_id', 'title', 'url', 'text_bad', 'incident_date',\n",
       "       'category', 'allegation_name', 'tag', 'nudity_penetration',\n",
       "       'sexual_relations_with_a_minor_', 'sexual_harassment_sexual_remarks',\n",
       "       'domestic_violence_police_committing_',\n",
       "       'sexual_humiliation_sexual_extortion_prostitution_sex_work',\n",
       "       'tasers_baton_aggressive_physical_touch_gun', 'trespass_robbery',\n",
       "       'biometric_surveillance_fitting_a_description_gang_related_',\n",
       "       'racial_slurs_xenophobic_remarks_',\n",
       "       'undocumented_status_asking_for_someone_s_status_calling_ice_',\n",
       "       'planting_drug_guns', 'neglect_of_duty_failure_to_serve',\n",
       "       'refusing_to_provide_medical_assistance', 'workplace_harassment',\n",
       "       '_irrational_aggressive_unstable_', 'suicide_in_jail_improper_care_',\n",
       "       'dcfs_threats', 'pregnant_women', 'school',\n",
       "       'searching_patting_down_arresting_minors', 'allegation_id', 'title.1',\n",
       "       'text_content', 'incident_date.1', 'most_common_category_id',\n",
       "       'category.1', 'allegation_name.1', 'coaccused_count', 'tag.1',\n",
       "       'source_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nudity_penetration\n",
      "0.010810810810810811\n",
      "\n",
      "sexual_relations_with_a_minor_\n",
      "0.0\n",
      "\n",
      "sexual_harassment_sexual_remarks\n",
      "0.010810810810810811\n",
      "\n",
      "domestic_violence_police_committing_\n",
      "0.013513513513513514\n",
      "\n",
      "sexual_humiliation_sexual_extortion_prostitution_sex_work\n",
      "0.010810810810810811\n",
      "\n",
      "tasers_baton_aggressive_physical_touch_gun\n",
      "0.34864864864864864\n",
      "\n",
      "trespass_robbery\n",
      "0.16216216216216217\n",
      "\n",
      "biometric_surveillance_fitting_a_description_gang_related_\n",
      "0.008108108108108109\n",
      "\n",
      "racial_slurs_xenophobic_remarks_\n",
      "0.07567567567567568\n",
      "\n",
      "undocumented_status_asking_for_someone_s_status_calling_ice_\n",
      "0.0\n",
      "\n",
      "planting_drug_guns\n",
      "0.06504065040650407\n",
      "\n",
      "neglect_of_duty_failure_to_serve\n",
      "0.03523035230352303\n",
      "\n",
      "refusing_to_provide_medical_assistance\n",
      "0.02168021680216802\n",
      "\n",
      "workplace_harassment\n",
      "0.0\n",
      "\n",
      "_irrational_aggressive_unstable_\n",
      "0.008130081300813009\n",
      "\n",
      "suicide_in_jail_improper_care_\n",
      "0.0027100271002710027\n",
      "\n",
      "dcfs_threats\n",
      "0.0\n",
      "\n",
      "pregnant_women\n",
      "0.0027100271002710027\n",
      "\n",
      "school\n",
      "0.0027100271002710027\n",
      "\n",
      "searching_patting_down_arresting_minors\n",
      "0.008130081300813009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Look at average of each category\n",
    "\n",
    "for cat in ['nudity_penetration',\n",
    "       'sexual_relations_with_a_minor_', 'sexual_harassment_sexual_remarks',\n",
    "       'domestic_violence_police_committing_',\n",
    "       'sexual_humiliation_sexual_extortion_prostitution_sex_work',\n",
    "       'tasers_baton_aggressive_physical_touch_gun', 'trespass_robbery',\n",
    "       'biometric_surveillance_fitting_a_description_gang_related_',\n",
    "       'racial_slurs_xenophobic_remarks_',\n",
    "       'undocumented_status_asking_for_someone_s_status_calling_ice_',\n",
    "       'planting_drug_guns', 'neglect_of_duty_failure_to_serve',\n",
    "       'refusing_to_provide_medical_assistance', 'workplace_harassment',\n",
    "       '_irrational_aggressive_unstable_', 'suicide_in_jail_improper_care_',\n",
    "       'dcfs_threats', 'pregnant_women', 'school',\n",
    "       'searching_patting_down_arresting_minors']:\n",
    "    print(cat)\n",
    "    print(training[cat].mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tasers_baton_aggressive_physical_touch_gun\n",
    "training['tasers_baton_aggressive_physical_touch_gun'] = training.apply(lambda x: 'pos' if x['tasers_baton_aggressive_physical_touch_gun'] == 1 else 'neg', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['text_content'] = training['text_content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "\n",
    "def remove_digits(row):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    return row['text_content'].translate(remove_digits)\n",
    "\n",
    "training['text_content'] = training.apply(remove_digits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1 = np.random.rand(len(training)) < 0.8\n",
    "train_dev = training[split_1]\n",
    "test_data = training[~split_1]\n",
    "split_2 = np.random.rand(len(train_dev)) < 0.6\n",
    "train_data = train_dev[split_2]\n",
    "dev_data = train_dev[~split_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(a[1], a[0]) for a in list(train_data[['tasers_baton_aggressive_physical_touch_gun', 'text_content']].to_records(index=False))]\n",
    "test_data = [(a[1], a[0]) for a in list(test_data[['tasers_baton_aggressive_physical_touch_gun', 'text_content']].to_records(index=False))]\n",
    "dev_data = [(a[1], a[0]) for a in list(dev_data[['tasers_baton_aggressive_physical_touch_gun', 'text_content']].to_records(index=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6BlH7U-iNX8",
    "outputId": "7e02c91d-6a1f-48f7-a076-416ba6793ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of TRAIN data 182\n",
      "number of TEST data 71\n",
      "number of DEV data 117\n"
     ]
    }
   ],
   "source": [
    "print(\"number of TRAIN data\", len(train_data))\n",
    "print(\"number of TEST data\", len(test_data))\n",
    "print(\"number of DEV data\", len(dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Sbm751RiNYN"
   },
   "source": [
    "#### We define a abstract model class as below. The model preforms preprocessing in its __init__ and has 2 functions, train and classify, which are implemented in subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NFRfRpaiNYR"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "class Model:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the training data\n",
    "        self.vocab = Counter([word for content, label in data for word in word_tokenize(content)]).most_common(VOCAB_SIZE-1) \n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in enumerate(self.vocab)} # word to index mapping\n",
    "        self.word_to_idx[\"UNK\"] = 0 # all the unknown words will be mapped to index 0\n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.idx_to_label = [POS_LABEL, NEG_LABEL]\n",
    "        self.vocab = set(self.word_to_idx.keys())\n",
    "        \n",
    "    def train_model(self, data):\n",
    "        '''\n",
    "        Train the model with the provided training data\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def classify(self, data):\n",
    "        '''\n",
    "        classify the documents with the model\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnhR3qF_iNYc"
   },
   "source": [
    "#### When training it helps to process multiple examples in a minibatch.  We shall use dataloading tools provided by PyTorch to create such minibatches.  The following class helps us interface with these PyTorch tools.\n",
    "\n",
    "You may optionally wish to see a tutorial on these tools: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LTwWPNy2iNYg"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    Our customized Dataset class.\n",
    "    '''\n",
    "    def __init__(self, word_to_idx, data):\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        bowvector = torch.zeros(self.vocab_size)\n",
    "        for word in word_tokenize(self.data[idx][0]):\n",
    "            bowvector[self.word_to_idx.get(word, 0)] += 1\n",
    "        label = self.label_to_idx[self.data[idx][1]]\n",
    "        return bowvector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tr-0xQveiNYn"
   },
   "outputs": [],
   "source": [
    "# We use early stopping to store the model that achieves that best performance\n",
    "# on DEV during training\n",
    "best_model = None\n",
    "\n",
    "class BoWLRClassifier(nn.Module, Model):\n",
    "    '''\n",
    "    Define your logistic regression model with bag of words features.\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        nn.Module.__init__(self)\n",
    "        Model.__init__(self, data)\n",
    "        \n",
    "        '''\n",
    "        In this model initialization phase, do the following: \n",
    "        1. Define a linear layer to transform bag of words features into 2 classes. \n",
    "        2. Define the loss function corresponding to cross entropy loss.\n",
    "           See https://pytorch.org/docs/stable/nn.html?highlight=crossen#torch.nn.CrossEntropyLoss\n",
    "        3. Define an optimizer for the model.  Choose SGD or Adam.\n",
    "            https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD\n",
    "        '''\n",
    "        self.linear = nn.Linear(len(self.word_to_idx), 2)  ##YOUR CODE HERE##\n",
    "        self.loss_function = nn.CrossEntropyLoss() ##YOUR CODE HERE##\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=.1) ##YOUR CODE HERE##\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run the model, which is just the linear layer and return the output.\n",
    "        '''\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def train_epoch(self, train_data):\n",
    "        '''\n",
    "        Train the model for one epoch (iterate through each example once).\n",
    "        '''\n",
    "        # For each minibatch:\n",
    "        # make a forward pass to get predictions\n",
    "        # compute loss using predictions and true y\n",
    "        # make a backward pass to get gradients\n",
    "        # update parameters by calling optimizer step\n",
    "        \n",
    "        dataset = TextClassificationDataset(self.word_to_idx, train_data)\n",
    "        dataloader = tud.DataLoader(dataset, batch_size=8)\n",
    "        self.train()\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x = x.float()\n",
    "            y = y.long()\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.forward(x) ##YOUR CODE HERE##\n",
    "            loss = self.loss_function(predictions, y)\n",
    "            loss.backward()\n",
    "            if i % 500 == 0:\n",
    "                print(f\"loss at {i}: {loss.item()}\")\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def train_model(self, train_data, dev_data):\n",
    "        \"\"\"\n",
    "        Train for multiple epochs and after each evaluate DEV accuracy.\n",
    "        Store the model with best DEV accuracy in best_model.\n",
    "        \"\"\"\n",
    "        global best_model\n",
    "        dev_accuracies = [0.]\n",
    "        highest_acc = 0\n",
    "        for epoch in range(10):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            self.train_epoch(train_data)\n",
    "            dev_acc = self.evaluate(dev_data)\n",
    "            print(f\"DEV accuracy: {dev_acc}\")\n",
    "            \n",
    "            # The following code copies the current model to best_model\n",
    "            # if the DEV accuracy is the best so far\n",
    "            if dev_acc > highest_acc: ##YOUR CODE HERE##\n",
    "                highest_acc = dev_acc\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accuracies.append(dev_acc)\n",
    "            \n",
    "    def evaluate(self, data):\n",
    "        '''\n",
    "        Compute the accuracy for data, i.e., the fraction of examples in \n",
    "        data for which the current model correctly predicts the class.\n",
    "        '''\n",
    "        \n",
    "        self.eval()\n",
    "        predictions = self.predict(data)\n",
    "        ys = [d[1] for d in data]\n",
    "        \n",
    "        correct = 0\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            if prediction == ys[i]:\n",
    "                correct += 1\n",
    "                \n",
    "        return correct / len(predictions)\n",
    "        \n",
    "                \n",
    "\n",
    "    def predict(self, data):\n",
    "        '''\n",
    "        Predict the classes for the examples in data.\n",
    "        '''\n",
    "        dataset = TextClassificationDataset(self.word_to_idx, data)\n",
    "        dataloader = tud.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(dataloader):\n",
    "                x = x.float()\n",
    "                if torch.cuda.is_available():\n",
    "                    x = x.cuda()\n",
    "                predictions = self.forward(x)\n",
    "                results.append(predictions.max(1)[1].cpu().numpy().reshape(-1))\n",
    "        results = np.concatenate(results)\n",
    "        results = [self.idx_to_label[p] for p in results]\n",
    "        return results\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GIZTEyjiNY0",
    "outputId": "8c98726f-a4d3-4839-b170-c7f6d61ba329",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [8 x 5000], m2: [4999 x 2] at C:\\w\\1\\s\\tmp_conda_3.7_100118\\conda\\conda-bld\\pytorch_1579082551706\\work\\aten\\src\\TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-b98402c8c167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlr_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# Uncomment this once your code is working properly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-9eccb0587026>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, train_data, dev_data)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mdev_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"DEV accuracy: {dev_acc}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-9eccb0587026>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, train_data)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##YOUR CODE HERE##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-9eccb0587026>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mRun\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mjust\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         '''\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [8 x 5000], m2: [4999 x 2] at C:\\w\\1\\s\\tmp_conda_3.7_100118\\conda\\conda-bld\\pytorch_1579082551706\\work\\aten\\src\\TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "train_d = train_data # Uncomment this once your code is working properly\n",
    "dev_d = dev_data\n",
    "\n",
    "lr_model = BoWLRClassifier(train_d)\n",
    "if torch.cuda.is_available():\n",
    "    lr_model = lr_model.cuda()\n",
    "lr_model.train_model(train_d, dev_d)\n",
    "best_model.evaluate(test_data ) # Uncomment this once your code is working properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eo-uTrWliNZM"
   },
   "source": [
    "#### Identify the top 10 features with the maximum weights for POSITIVE category. Explain your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were the 10 words which had the highest weights indicating label=0 which was POSITIVE, meaning that these words were associated with positive movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, idxs = torch.topk(best_model.state_dict()['linear.weight'][0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['excellent',\n",
       " 'great',\n",
       " 'best',\n",
       " 'beautiful',\n",
       " 'job',\n",
       " 'wonderful',\n",
       " 'perfect',\n",
       " 'both',\n",
       " 'loved',\n",
       " 'amazing']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[best_model.idx_to_word[a] for a in idxs.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pk7fZ5vQiNZW"
   },
   "source": [
    "#### Identify the top 10 features with the maximum negative weights for POSITIVE category. Explain your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were the 10 words which had the lowest weights indicating label=0 which was POSITIVE, meaning that these words were least associated with positive movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWfPmouZiNZX",
    "outputId": "aa9f5a0d-f09b-4c9b-9df9-03061956f1d6"
   },
   "outputs": [],
   "source": [
    "values, idxs = torch.topk(best_model.state_dict()['linear.weight'][0], 10, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worst',\n",
       " 'waste',\n",
       " 'poor',\n",
       " 'nothing',\n",
       " 'bad',\n",
       " 'script',\n",
       " 'poorly',\n",
       " 'boring',\n",
       " 'awful',\n",
       " 'supposed']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[best_model.idx_to_word[a] for a in idxs.tolist()]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw1-solution_collab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
